---
name: ðŸ•·ï¸ Web Crawler Dev Plan
about: Track the 10-week roadmap for building and publishing a Python-based web crawler
title: "[TRACKER] Web Crawler Development Roadmap"
labels: enhancement, project
assignees: swayamdani

---

## ðŸ“… Weekly Tasks

- [ ] **Week 1 (June 17â€“23):** Learn HTTP basics & fetch pages using `http.client` or `requests`
- [ ] **Week 2 (June 24â€“30):** Extract links from raw HTML using regex or string parsing
- [ ] **Week 3 (July 1â€“7):** Normalize URLs and implement basic crawling with depth limit
- [ ] **Week 4 (July 8â€“14):** Add visited set, domain restriction, and basic logging
- [ ] **Week 5 (July 15â€“21):** Implement robots.txt parser and respect disallowed paths
- [ ] **Week 6 (July 22â€“28):** Add CLI options for depth, domain, output format
- [ ] **Week 7 (July 29â€“Aug 4):** Refactor code, split into modules, and write documentation
- [ ] **Week 8 (Aug 5â€“11):** Add `setup.py`, README, license, and prep for PyPI
- [ ] **Week 9 (Aug 12â€“18):** Write tests, finalize CLI, and publish to GitHub
- [ ] **Week 10 (Aug 19â€“25):** Upload to PyPI and announce project

---

## ðŸ›¡ï¸ Repo Security Setup Checklist

- [ ] Enable branch protection rules (require PR reviews, no direct `main` pushes)
- [ ] Add `.gitignore` (for `__pycache__`, `.env`, etc.)
- [ ] Set up `LICENSE` (MIT recommended)
- [ ] Add `SECURITY.md` for responsible disclosure
- [ ] Set Dependabot alerts (GitHub Settings > Security > Enable alerts)
- [ ] Use `bandit` or `pylint` for security linting (CI)
- [ ] Restrict GitHub Actions to reviewed workflows only

---

## ðŸ“‚ Suggested Repo Structure

```bash
webcrawler/
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py
â”‚   â”œâ”€â”€ utils.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_core.py
â”œâ”€â”€ cli.py
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ SECURITY.md
```

