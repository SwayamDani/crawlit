.. crawlit documentation master file, created by
   sphinx-quickstart on Wed May 14 12:30:54 2025.

Welcome to crawlit's documentation!
===================================

.. image:: https://img.shields.io/badge/License-MIT-yellow.svg
   :target: https://opensource.org/licenses/MIT
   :alt: License: MIT
.. image:: https://img.shields.io/badge/python-3.8+-blue.svg
   :target: https://www.python.org/downloads/
   :alt: Python 3.8+

A powerful, modular, and ethical web crawler built in Python. Designed for security testing, link extraction, 
and website structure mapping with a focus on clean architecture and extensibility.

Features
--------

* **Modular Architecture**: Easily extend with custom modules and parsers
* **Ethical Crawling**: Configurable robots.txt compliance and rate limiting
* **Depth Control**: Set maximum crawl depth to prevent excessive resource usage
* **Domain Filtering**: Restrict crawling to specific domains or subdomains
* **Robust Error Handling**: Gracefully manage connection issues and malformed pages
* **Multiple Output Formats**: Export results as JSON, CSV, or plain text
* **Detailed Logging**: Comprehensive logging of all crawler activities
* **Command Line Interface**: Simple, powerful CLI for easy usage
* **Programmatic API**: Use as a library in your own Python code

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   installation
   quickstart
   usage
   api/index
   examples
   contributing
   changelog

